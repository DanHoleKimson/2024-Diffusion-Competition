{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import (Dataset, DataLoader)\n",
    "\n",
    "import torchvision\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from typing import Dict, List, Tuple\n",
    "import wandb\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 CLASSES의 이름은 다운 받은 폴더의 이름이 일치해야 합니다. ex) data/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리가 분류할 4개의 클래스\n",
    "#CLASSES = [ 'MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented' ]\n",
    "classes = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "config들."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_DEVICES = torch.cuda.device_count()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    NUM_CLASSES = 4\n",
    "    EPOCHS = 16\n",
    "    BATCH_SIZE = (\n",
    "        32 if torch.cuda.device_count() < 2 \n",
    "        else (32 * torch.cuda.device_count())\n",
    "    )\n",
    "    LR = 0.001\n",
    "    APPLY_SHUFFLE = True\n",
    "    SEED = 768\n",
    "    HEIGHT = 224\n",
    "    WIDTH = 224\n",
    "    CHANNELS = 3\n",
    "    IMAGE_SIZE = (224, 224, 3)\n",
    "    \n",
    "    # Define paths\n",
    "    DATASET_PATH = \"/kaggle/input/brain-tumor-mri-dataset/\"\n",
    "    TRAIN_PATH = '/kaggle/input/brain-tumor-mri-dataset/Training/'\n",
    "    TEST_PATH = '/kaggle/input/brain-tumor-mri-dataset/Testing/'\n",
    "    \n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습에 필요한 상수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 학습에 필요한 상수들을 정의 합니다.\n",
    "IMG_SHAPE = (224, 224, 3)\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "\n",
    "# 각 이미지의 기본 주소\n",
    "BASE_PATH = './drive/MyDrive/Colab Notebooks/data/'\n",
    "images_dir = Path(BASE_PATH).expanduser()\n",
    "print(images_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "내 드라이브 파일 마운트하고, 폴더 정보 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# 모델 학습에 사용할 데이터 정보를 설정합니다.\n",
    "class_list = []\n",
    "num_list = []\n",
    "\n",
    "# IMAGE_BASE_PATH = './data/'\n",
    "train_path = BASE_PATH + 'train/'\n",
    "for folder in os.listdir(train_path):\n",
    "    classes.append(folder)\n",
    "    folder_size = len(os.listdir(train_path+folder))\n",
    "    class_list.append(folder)\n",
    "    num_list.append(folder_size)\n",
    "\n",
    "voc_s = pd.Series(num_list,index=class_list)\n",
    "voc_s.sort_values().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "print(voc_s.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 검증에 사용할 데이터 내용 입니다.\n",
    "\n",
    "class_list = []\n",
    "num_list = []\n",
    "\n",
    "#IMAGE_BASE_PATH = './data/'\n",
    "valid_path = BASE_PATH + 'test/'\n",
    "for folder in os.listdir(valid_path):\n",
    "    folder_size = len(os.listdir(valid_path+folder))\n",
    "    # print('{:<15} : {}'.format(folder,folder_size))\n",
    "    class_list.append(folder)\n",
    "    num_list.append(folder_size)\n",
    "\n",
    "voc_s = pd.Series(num_list,index=class_list)\n",
    "voc_s.sort_values().plot(kind='bar')\n",
    "plt.show()\n",
    "\n",
    "print(voc_s.sort_values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = glob.glob(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "test_images = glob.glob(f\"{CFG.TEST_PATH}**/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(image_paths):\n",
    "    return [(_.split('/')[-2:][0]).replace('-', '_') for _ in image_paths]\n",
    "\n",
    "def build_df(image_paths, labels):\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'label': generate_labels(labels)\n",
    "    })\n",
    "    \n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the DataFrames\n",
    "# train_df = build_df(train_images, generate_labels(train_images))\n",
    "train_df = build_df(train_images, train_images)\n",
    "# test_df = build_df(test_images, generate_labels(test_images))\n",
    "test_df = build_df(test_images, test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions for display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(image_path, as_tensor=True):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if as_tensor:\n",
    "        converter = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Grayscale()\n",
    "        ])\n",
    "        return converter(image)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "def view_sample(image, label, color_map='rgb', fig_size=(8, 10)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    if color_map=='rgb':\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        plt.imshow(image, cmap=color_map)\n",
    "    \n",
    "    plt.title(f'Label: {label}', fontsize=16)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Train/Val split with Training Set\n",
    "train_split_idx, val_split_idx, _, _ = (\n",
    "    train_test_split(\n",
    "        train_df.index, \n",
    "        train_df.label, \n",
    "        test_size=0.20,\n",
    "        stratify=train_df.label,\n",
    "        random_state=CFG.SEED\n",
    "    )\n",
    ")\n",
    "\n",
    "# Get training and remaining data\n",
    "train_new_df = train_df.iloc[train_split_idx].reset_index(drop=True)\n",
    "val_df = train_df.iloc[val_split_idx].reset_index(drop=True)\n",
    "\n",
    "# View shapes\n",
    "# train_new_df.shape, val_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment train data\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.5),\n",
    "#     transforms.RandomRotation(degrees=30,),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Only reshape test data\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((CFG.HEIGHT, CFG.WIDTH)),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BrainTumorMRIDataset(Dataset):\n",
    "    def __init__(self, df:pd.DataFrame, transform=None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.paths = df['image_path'].to_list()\n",
    "        self.labels = df['label'].to_list()\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.classes = sorted(list(df['label'].unique()))\n",
    "        self.class_to_idx = {cls_name: _ for _, cls_name in enumerate(self.classes)}\n",
    "        \n",
    "    def load_image(self, index:int) -> Image.Image:\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path).convert('RGB')  \n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.paths.__len__()\n",
    "    \n",
    "    def __getitem__(self, index:int) -> Tuple[torch.Tensor, int]:\n",
    "        image = self.load_image(index)\n",
    "        class_name = self.labels[index]\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "            return self.transform(image), class_idx\n",
    "        else:\n",
    "            return image, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train dataset\n",
    "train_ds = BrainTumorMRIDataset(train_new_df, transform=train_transforms)\n",
    "\n",
    "# Build validation dataset\n",
    "val_ds = BrainTumorMRIDataset(val_df, transform=test_transforms)\n",
    "\n",
    "# Build test dataset\n",
    "test_ds = BrainTumorMRIDataset(test_df, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build DataLoaders from Custom Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build train dataloader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_ds, \n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=CFG.APPLY_SHUFFLE\n",
    ")\n",
    "\n",
    "# Build validation dataloader\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_ds, \n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Build test dataloader\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_ds, \n",
    "    batch_size=CFG.BATCH_SIZE,\n",
    "    num_workers=CFG.NUM_WORKERS,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transfer Learning 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Model(nn.Module):\n",
    "    def __init__(self, backbone_model, name='efficientnet-v2-large', \n",
    "                 num_classes=CFG.NUM_CLASSES, device=CFG.DEVICE):\n",
    "        super(EfficientNetV2Model, self).__init__()\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.name = name\n",
    "        \n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(in_features=1280, out_features=256, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=num_classes, bias=False)\n",
    "        ).to(device)\n",
    "        \n",
    "        self._set_classifier(classifier)\n",
    "        \n",
    "    def _set_classifier(self, classifier:nn.Module) -> None:\n",
    "        self.backbone_model.classifier = classifier\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.backbone_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effiecientnetv2_model(\n",
    "    device: torch.device=CFG.NUM_CLASSES) -> nn.Module:\n",
    "    # Set the manual seeds\n",
    "    torch.manual_seed(CFG.SEED)\n",
    "    torch.cuda.manual_seed(CFG.SEED)\n",
    "\n",
    "    # Get model weights\n",
    "    model_weights = (\n",
    "        torchvision.models.EfficientNet_V2_L_Weights.DEFAULT)\n",
    "    \n",
    "    # Get model and push to device\n",
    "    model = (\n",
    "        torchvision.models.efficientnet_v2_l(\n",
    "            weights=model_weights\n",
    "        )\n",
    "    ).to(device) \n",
    "    \n",
    "    # Freeze Model Parameters\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EfficientNet v2 model\n",
    "backbone_model = get_effiecientnetv2_model(CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetv2_params = {\n",
    "    'backbone_model'    : backbone_model,\n",
    "    'name'              : 'efficientnet-v2-large',\n",
    "    'device'            : CFG.DEVICE\n",
    "}\n",
    "\n",
    "# Generate Model\n",
    "efficientnet_model = EfficientNetV2Model(**efficientnetv2_params)\n",
    "\n",
    "# If using GPU T4 x2 setup, use this:\n",
    "if CFG.NUM_DEVICES > 1:\n",
    "    efficientnet_model = nn.DataParallel(efficientnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View model summary\n",
    "summary(\n",
    "    model=efficientnet_model, \n",
    "    input_size=(CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- define Loss & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function\n",
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    label_smoothing=0.1\n",
    ")\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    efficientnet_model.parameters(),\n",
    "    lr=CFG.LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Epoch Execution (Train Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_epoch(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    # Set model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize train loss & accuracy\n",
    "    train_loss, train_acc = 0, 0\n",
    "    \n",
    "    # Execute training loop over train dataloader\n",
    "    for batch, (X, y) in enumerate(tqdm(dataloader)):\n",
    "        # Load data onto target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        # Feed-forward and compute metrics\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "        \n",
    "        # Reset Gradients & Backpropagate Loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Model Gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute Batch Metrics\n",
    "        predicted_class = torch.argmax(\n",
    "            torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "        \n",
    "    # Compute Step Metrics\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    \n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Evaluation Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model:torch.nn.Module,\n",
    "    dataloader:torch.utils.data.DataLoader,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    device:torch.device) -> Tuple[float, float]:\n",
    "    \n",
    "    # Set model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Initialize eval loss & accuracy\n",
    "    eval_loss, eval_acc = 0, 0\n",
    "    \n",
    "    # Active inferene context manager\n",
    "    with torch.inference_mode():\n",
    "        # Execute eval loop over dataloader\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Load data onto target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Feed-forward and compute metrics\n",
    "            y_pred = model(X)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            eval_loss += loss.item() \n",
    "\n",
    "            # Compute Batch Metrics\n",
    "            predicted_class = torch.argmax(\n",
    "                torch.softmax(y_pred, dim=1), dim=1)\n",
    "            eval_acc += (predicted_class == y).sum().item() / len(y_pred)\n",
    "        \n",
    "    # Compute Step Metrics\n",
    "    eval_loss = eval_loss / len(dataloader)\n",
    "    eval_acc = eval_acc / len(dataloader)\n",
    "    \n",
    "    return eval_loss, eval_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    model:torch.nn.Module,\n",
    "    train_dataloader:torch.utils.data.DataLoader,\n",
    "    eval_dataloader:torch.utils.data.DataLoader,\n",
    "    optimizer:torch.optim.Optimizer,\n",
    "    loss_fn:torch.nn.Module,\n",
    "    epochs:int,\n",
    "    device:torch.device) -> Dict[str, List]:\n",
    "    \n",
    "    # `wandb` 초기화\n",
    "    wandb.init(project=\"2024_Hackaton\", entity=\"tjwjddn980117\")\n",
    "\n",
    "    # 모델과 옵티마이저를 `wandb`에 로깅하기 위해 추가\n",
    "    wandb.watch(model, log=\"all\")\n",
    "\n",
    "    # 최고 평가 정확도를 추적하기 위한 변수 초기화\n",
    "    best_eval_accuracy = 0.0\n",
    "\n",
    "    # Initialize training session\n",
    "    session = {\n",
    "        'loss'          : [],\n",
    "        'accuracy'      : [],\n",
    "        'eval_loss'     : [],\n",
    "        'eval_accuaracy': []\n",
    "    }\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        # Execute Epoch\n",
    "        print(f'\\nEpoch {epoch + 1}/{epochs}')\n",
    "        train_loss, train_acc = execute_epoch(\n",
    "            model, \n",
    "            train_dataloader, \n",
    "            optimizer, \n",
    "            loss_fn, \n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Evaluate Model\n",
    "        eval_loss, eval_acc = evaluate(\n",
    "            model, \n",
    "            eval_dataloader,\n",
    "            loss_fn, \n",
    "            device\n",
    "        )\n",
    "        \n",
    "        # Log Epoch Metrics\n",
    "        print(f'loss: {train_loss:.4f} - acc: {train_acc:.4f} - eval_loss: {eval_loss:.4f} - eval_acc: {eval_acc:.4f}')\n",
    "        \n",
    "        # Record Epoch Metrics\n",
    "        session['loss'].append(train_loss)\n",
    "        session['accuracy'].append(train_acc)\n",
    "        session['eval_loss'].append(eval_loss)\n",
    "        session['eval_accuaracy'].append(eval_acc)\n",
    "        \n",
    "        # Check for improvement\n",
    "        if eval_acc > best_eval_accuracy:\n",
    "            print(f\"새로운 최고 평가 정확도: {eval_acc:.4f}, 이전: {best_eval_accuracy:.4f}\")\n",
    "            best_eval_accuracy = eval_acc\n",
    "            # 모델 가중치 저장\n",
    "            torch.save(model.state_dict(), 'best_model_weights.pth')\n",
    "            print(\"모델 가중치가 'best_model_weights.pth'에 저장되었습니다.\")\n",
    "            \n",
    "    # Return Session Metrics\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train EfficientNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model \n",
    "print('Training EfficientNet Model')\n",
    "print(f'Train on {len(train_new_df)} samples, validate on {len(val_df)} samples.')\n",
    "print('----------------------------------')\n",
    "\n",
    "efficientnet_session_config = {\n",
    "    'model'               : efficientnet_model,\n",
    "    'train_dataloader'    : train_loader,\n",
    "    'eval_dataloader'     : val_loader,\n",
    "    'optimizer'           : optimizer,\n",
    "    'loss_fn'             : loss_fn,\n",
    "    'epochs'              : CFG.EPOCHS,\n",
    "    'device'              : CFG.DEVICE\n",
    "}\n",
    "\n",
    "efficientnet_session_history = train(**efficientnet_session_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    model:nn.Module, \n",
    "    sample_loader:torch.utils.data.DataLoader,\n",
    "    device:torch.device) -> np.ndarray:\n",
    "    \n",
    "    # Set model into eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    # Active inferene context manager\n",
    "    with torch.inference_mode():\n",
    "        # Execute eval loop over dataloader\n",
    "        for batch, (X, y) in enumerate(tqdm(sample_loader)):\n",
    "            # Load data onto target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Feed-forward and compute metrics\n",
    "            y_pred = model(X) \n",
    "\n",
    "            # Compute Batch Metrics\n",
    "            predicted_probs = torch.softmax(y_pred, dim=1).cpu().numpy()\n",
    "            \n",
    "            # Record prediction\n",
    "            predictions.append(predicted_probs)\n",
    "        \n",
    "    return np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test sample probabilities \n",
    "efficientnet_test_probs = predict(efficientnet_model, test_loader, CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test sample preditions \n",
    "efficientnet_test_preds = np.argmax(efficientnet_test_probs, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hackaton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
